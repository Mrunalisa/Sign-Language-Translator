# Sign Language Translator

A real-time **Sign Language Translator** built using **Python**, **MediaPipe Holistic**, **OpenCV**, and **Random Forest Classifier**.
This project captures hand gestures via a webcam, processes them into hand landmarks, and translates recognized signs into **text**â€”bridging communication gaps for people with hearing or speech impairments.

---

## âœ¨ Features

* ğŸ“¸ **Real-time hand tracking** using Googleâ€™s **MediaPipe Holistic**
* ğŸ§  **Random Forest Classifier** for robust gesture recognition
* ğŸ”¤ Supports **American Sign Language (ASL)**
* âš¡ **Fast & lightweight** â€“ runs on a normal laptop webcam
* ğŸ’¾ Easy dataset creation using image collection scripts
* ğŸ–¥ï¸ Text output for recognized signs (future extension: speech output)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ README.md
â””â”€â”€ ASL/
    â”œâ”€â”€ data/                        # Collected gesture images
    â”œâ”€â”€ collect_imgs.py              # Capture gesture images
    â”œâ”€â”€ collect_imgs_for-e-ilu.py    # Collect specific signs (e,ilu)
    â”œâ”€â”€ create_dataset.py            # Build dataset (pickle format)
    â”œâ”€â”€ data.pickle                  # Serialized dataset (features + labels)
    â”œâ”€â”€ train_classifier.py          # Train Random Forest Classifier
    â”œâ”€â”€ model.p                      # Trained model file
    â”œâ”€â”€ inference_classifier.py      # Real-time gesture recognition
    â””â”€â”€ requirements.txt             # Dependencies
```

---

## ğŸ› ï¸ Tech Stack

* **Python 3.8+**
* **OpenCV** â†’ Image capture & preprocessing
* **MediaPipe Holistic** â†’ Extract 21 hand landmarks
* **Scikit-learn** â†’ Random Forest Classifier
* **NumPy / Pickle** â†’ Data handling & model persistence

---

## ğŸš€ How It Works

1. **Data Collection**
   Capture gesture images using `collect_imgs.py`. Images are stored in `data/` by label.

2. **Dataset Preparation**
   Run `create_dataset.py` to extract MediaPipe hand landmarks â†’ store in `data.pickle`.

3. **Model Training**
   Train a Random Forest Classifier with `train_classifier.py`. The model is saved as `model.p`.

4. **Real-time Inference**
   Run `inference_classifier.py` to open webcam â†’ system detects hand landmarks â†’ predicts the corresponding sign â†’ displays it as **text**.

---

## âš™ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/<your-username>/sign-language-translator.git
cd sign-language-translator/ASL

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows

# Install dependencies
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### 1. Collect Gesture Images

```bash
python collect_imgs.py
```

### 2. Create Dataset

```bash
python create_dataset.py
```

### 3. Train the Model

```bash
python train_classifier.py
```

### 4. Run Real-time Translator

```bash
python inference_classifier.py
```

---

## ğŸ“Š Example Results

* **Gesture: Hello (ASL)** â†’ Output: `Hello`
* **Gesture: L (ASL)** â†’ Output: `L`


(Future: Expand vocabulary to full alphabets, words, and phrases)

---


## ğŸ”® Future Scope

* Currently, the system supports **ASL gestures**; in the future, **Indian Sign Language (ISL)** can also be implemented for broader applicability.
* Expand dataset with more gestures (alphabets, words, sentences).
* Add **speech output** along with text for better accessibility.
* Support **two-hand detection** required for ISL gestures.
* Improve robustness under varied lighting & backgrounds.
* Explore **deep learning (CNN/LSTM)** for recognizing continuous/dynamic signs.


---

## ğŸ™ Acknowledgements

* [Google MediaPipe](https://github.com/google/mediapipe)
* [OpenCV](https://opencv.org/)
* [Scikit-learn](https://scikit-learn.org/)

---

## ğŸ“œ License

This project is licensed under the MIT License


